# PART A ASSIGNMENT 2
#!pip install datasets
import datasets
from datasets import load_dataset
import random
from nltk import ngrams

# Load Wikipedia Simple dataset
dataset = load_dataset("wikipedia", "20220301.simple")

# Extract text data from the dataset
corpus = dataset['train']['text'][:10000]

# Create a list of all words in the corpus
words = ' '.join(corpus).split()

# Generate sentences using a unigram language model
def generate_sentence_unigram(corpus, num_words=10):
    sentence = []
    for _ in range(num_words):
        word = random.choice(corpus)
        sentence.append(word)
    return ' '.join(sentence)

# Generate and print 5 sentences
sentences = []
temp =""
for i in range(5):
  temp = generate_sentence_unigram(words)
  sentences.append(temp)
  print(f"Sentence {i + 1}: {temp}")

print("part bbbbbbbbbbbbb")

n = 2
for i in range(5): 
  sentence = '<s>' +sentences[i]+'</s>'
  unigrams = ngrams(sentence.split(), n)
  print("строка \n")
  for grams in unigrams:print(grams)


print("part ccccccccccccccccc \n 4 grams")
n = 4
for i in range(5): 
  sentence = '<s>' +sentences[i]+'</s>'
  unigrams = ngrams(sentence.split(), n)
  print("строка \n")
  for grams in unigrams:print(grams)


print("5 grams")
n = 5
for i in range(5): 
  sentence = '<s>' +sentences[i]+'</s>'
  unigrams = ngrams(sentence.split(), n)
  print("строка \n")
  for grams in unigrams:print(grams)
